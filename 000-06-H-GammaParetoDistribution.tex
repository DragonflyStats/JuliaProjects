\documentclass[]{report}

\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 480pt

\usepackage{framed}
\usepackage{subfiles}
\usepackage{graphics}
\usepackage{newlfont}
\usepackage{eurosym}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\begin{document}


\section{Other useful Continuous Distributions}
\begin{description}
\item [Pareto distribution] for a single such quantity whose log is exponentially distributed; 
the prototypical power law distribution
\item [Log-normal distribution] for a single such quantity whose log is normally distributed
\item [Weibull distribution]
\end{description}




\noindent \textbf{ Continuous Distributions}
\begin{multicols}{2}
\begin{itemize}
\item[(a)] The continuous uniform distribution
\item[(b)] The exponential distribution
\item[(c)] The Weibull distribution
\item[(d)] The Pareto distribution
\end{itemize}
\end{multicols}



\section{Gamma Distribution}




The Gamma distribution is very important for technical reasons, since it is the parent of the exponential distribution and can explain many other distributions.

The probability distribution function is:

$$
f_x (x) =
\begin{cases}
\frac{1}{a^p \Gamma (p)} x^{p-1} e^{-x/a}, & \mbox{if } x \ge 0 \\
0, & \mbox{if } x < 0
\end{cases}\quad a,p >0$$

Where  $\Gamma(p) = \int_0^\infty  t^{p-1} e^{-t}\,dt\,$ is the Gamma function. The cumulative distribution function cannot be found unless p=1, in which case the Gamma distribution becomes the exponential distribution. The Gamma distribution of the stochastic variable X is denoted as  $X \in \Gamma (p,a)$ .

Alternatively, the gamma distribution can be parameterized in terms of a shape parameter $\alpha = k$ and an inverse scale parameter $\beta = 1/\theta$, called a rate parameter:

\[ g(x;\alpha,\beta) = K x^{\alpha-1}  e^{-\beta\,x}   \ \mathrm{for}\ x > 0 \,\!.\]
where the K constant can be calculated setting the integral of the density function as 1:


\[\int_{-\infty}^{+\infty}g(x;\alpha,\beta) \mathrm{d}t \, = \int_{0}^{+\infty} K x^{\alpha-1}  e^{-\beta\,x}  \mathrm{d}x \, = 1\]
following:

\[
K \int_{0}^{+\infty} x^{\alpha-1}  e^{-\beta\,x}  \mathrm{d}x \, = 1\]

\[K = \frac{1}{\int_{0}^{+\infty} x^{\alpha-1}  e^{-\beta\,x}  \mathrm{d}x}\]
and, with change of variable $ y = \beta x  :$


\begin{align}
K &= \frac{1}{\int_{0}^{+\infty} \frac{y^{\alpha-1}}{\beta^{\alpha - 1}}  e^{-y}  \frac{\mathrm{d}y}{\beta}} \\
&= \frac{1}{\frac{1}{\beta^{\alpha}}\int_{0}^{+\infty} y^{\alpha-1}  e^{-y} \mathrm{d}y} \\
&= \frac{\beta^{\alpha}}{\int_{0}^{+\infty} y^{\alpha-1}  e^{-y}  \mathrm{d}y} \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)}
\end{align}

following:

\[ g(x;\alpha,\beta) = x^{\alpha-1}  \frac{\beta^{\alpha} \, e^{-\beta\,x} }{\Gamma(\alpha)}  \ \mathrm{for}\ x > 0 \,\!.\]

%===================================================================================%
\subsection{Probability Density Function}
We first check that the total integral of the probability density function is 1.

$$\int^\infty_{-\infty}\frac{1}{a^p \Gamma (p)} x^{p-1} e^{-x/a}dx$$
Now we let y=x/a which means that dy=dx/a

$$\frac{1}{ \Gamma (p)} \int^\infty_{0} y^{p-1} e^{-y}dy$$
$$\frac{1}{ \Gamma (p)} \Gamma (p)=1$$
%===================================================================================%
Mean
\[\operatorname{E}[X]=\int^\infty_{-\infty}x \cdot \frac{1}{a^p \Gamma (p)} x^{p-1} e^{-x/a}dx\]
Now we let y=x/a which means that dy=dx/a.

\[\operatorname{E}[X]=\int^\infty_{0}ay \cdot \frac{1}{\Gamma (p)} y^{p-1} e^{-y}dy\]
$$\operatorname{E}[X]=\frac{a}{\Gamma (p)}\int^\infty_{0}y^{p} e^{-y}dy$$
$$\operatorname{E}[X]=\frac{a}{\Gamma (p)}\Gamma (p+1)$$
We now use the fact that $\Gamma (z+1)=z\Gamma (z)$

\[\operatorname{E}[X]=\frac{a}{\Gamma (p)}p\Gamma (p)=ap\]

%===================================================================================%
Variance[edit]
We first calculate $E[X^2]$

$$\operatorname{E}[X^2]=\int^\infty_{-\infty}x^2 \cdot \frac{1}{a^p \Gamma (p)} x^{p-1} e^{-x/a}dx$$
Now we let y=x/a which means that dy=dx/a.

$$\operatorname{E}[X^2]=\int^\infty_0 a^2 y^2 \cdot \frac{1}{a \Gamma (p)} y^{p-1} e^{-y}ady$$
$$\operatorname{E}[X^2]=\frac{a^2}{ \Gamma (p)}\int^\infty_0  y^{p+1} e^{-y}dy$$
$$\operatorname{E}[X^2]=\frac{a^2}{ \Gamma (p)}\Gamma (p+2) =pa^2(p+1)$$
Now we use calculate the variance

$$\operatorname{Var}(X)=\operatorname{E}[X^2]-(\operatorname{E}[X])^2$$
$$\operatorname{Var}(X)=pa^2(p+1)-(ap)^2=pa^2$$


Gamma Distribution[edit]
Gamma
Probability density function
Probability density plots of gamma distributions
Cumulative distribution function
Cumulative distribution plots of gamma distributions
Parameters

\begin{verbatim}
\scriptstyle k \;>\; 0 shape
\scriptstyle \theta \;>\; 0\, scale
Support\scriptstyle x \;\in\; (0,\, \infty)\!
PDF\scriptstyle \frac{1}{\Gamma(k) \theta^k} x^{k \,-\, 1} e^{-\frac{x}{\theta}}\,\!
CDF\scriptstyle \frac{1}{\Gamma(k)} \gamma\left(k,\, \frac{x}{\theta}\right)\!
Mean\scriptstyle \operatorname{E}[ X] = k \theta \!
\scriptstyle \operatorname{E}[\ln X] = \psi(k) +\ln(\theta)\!
(see digamma function)
MedianNo simple closed form
Mode\scriptstyle (k \,-\, 1)\theta \text{ for } k \;>\; 1\,\!
Variance\scriptstyle\operatorname{Var}[ X] = k \theta^2\,\!
\scriptstyle\operatorname{Var}[\ln X] = \psi_1(k)\!
(see trigamma function )
Skewness\scriptstyle \frac{2}{\sqrt{k}}\,\!
Ex. kurtosis\scriptstyle \frac{6}{k}\,\!
Entropy\scriptstyle \begin{align}
\scriptstyle k &\scriptstyle \,+\, \ln\theta \,+\, \ln[\Gamma(k)]\\
\scriptstyle   &\scriptstyle \,+\, (1 \,-\, k)\psi(k)
\end{align}
\end{verbatim}


\newpage
\section{Gamma Distribution}

\textbf{Applications}\\
The gamma distribution can be used a range of disciplines including queuing models, climatology, and financial services. 
%Examples of events that may be modeled by gamma distribution include:
\begin{itemize}
\item The amount of rainfall accumulated in a reservoir
\item The size of loan defaults or aggregate insurance claims
\item The flow of items through manufacturing and distribution processes
\item The load on web servers
%\item The many and varied forms of telecom exchange
\end{itemize}

\section{Other useful Continuous Distributions}
%http://www.computing.dcu.ie/~jhorgan/chapter16slides.pdf

%Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations)[edit]
%---------------------------------------------------------------------------------%




\subsection{Gamma Distribution}

There are three different parametrizations in common use: \bigskip
\begin{itemize}
\item With a shape parameter k and a scale parameter $\theta$. \bigskip
\item With a shape parameter $\alpha = k$ and an inverse scale parameter $\beta = 1/\theta$, called a rate parameter. \bigskip
\item With a shape parameter k and a mean parameter $\mu = k/\beta$.
\end{itemize}

\noindent \textbf{Gamma Distribution: Probability density function (pdf)}

\[{\frac {1}{\Gamma (k)\theta ^{k}}}x^{k\,-\,1}e^{-{\frac {x}{\theta }}}\]

\[{\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}x^{\alpha \,-\,1}e^{-\beta x}\]
\newpage

\section{The General Pareto Distribution}

\begin{itemize}
\item As with many other distributions, the Pareto distribution is often generalized by adding a scale parameter. Thus, suppose that Z has the basic Pareto distribution with shape parameter a>0.
\item If b>0, the random variable X=bZ has the Pareto distribution with shape parameter a and scale parameter b. Note that X takes values in the interval $[b, \infty)$.

\item Analogies of the results given above follow easily from basic properties of the scale transformation.

\end{itemize}


%%- \subsection{The Pareto Distribution}

\begin{itemize}
\item The probability density function is
\[ f(x)=abaxa+1,b\leq x< \infty \]
\item The distribution function is
\[F(x)=1-(bx)a,b\leq x< \infty \]


\item The quantile function is
\[F-1(p)=b(1-p)1/a,0\leq p<1\]
\end{itemize}


\subsection{Cumulative distribution function}
From the definition, the cumulative distribution function of a Pareto random variable with parameters $\alpha$ and xm is
\[F_X(x) = \begin{cases}
1-\left(\frac{x_\mathrm{m}}{x}\right)^\alpha & \mbox{for } x \ge x_\mathrm{m}, \\
0 & \mbox{for }x < x_\mathrm{m}.
\end{cases}
\]


When plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.
\subsection{Probability density function}
It follows (by differentiation) that the probability density function is
\[
f_X(x)= \begin{cases} \alpha\,\dfrac{x_\mathrm{m}^\alpha}{x^{\alpha+1}} & \mbox{for }x \ge x_\mathrm{m}, \\[12pt] 0 & \mbox{for } x < x_\mathrm{m}. \end{cases} 
\]



% Moments and characteristic function[edit]
The expected value of a random variable following a Pareto distribution is
\[
E(X)= \begin{cases} \infty & \text{if }\alpha\le 1, \\ \frac{\alpha x_\mathrm{m}}{\alpha-1} & \text{if }\alpha>1. \end{cases}
\]
The variance of a random variable following a Pareto distribution is
\[
\mathrm{Var}(X)= \begin{cases} \infty & \text{if }\alpha\in(1,2], \\ \left(\frac{x_\mathrm{m}}{\alpha-1}\right)^2 \frac{\alpha}{\alpha-2} & \text{if }\alpha>2. \end{cases}
(If \alpha\le 1, the variance does not exist.)
\]






The Pareto distribution is 
a continuous distribution with the probability density function (pdf):
\[
f(x; \alpha, \beta) = \alpha\beta\alpha / x\alpha+ 1
\]
For shape parameter $\alpha > 0$, and location parameter $\beta > 0$, and $\alpha > 0$.


% The following graph illustrates how the PDF varies with the location parameter \beta:

\subsection{Cumulative distribution function}

From the definition, the cumulative distribution function of a Pareto random variable with parameters $\alpha$ and $x_m$ is
\[F_X(x) = \begin{cases}
1-\left(\frac{x_\mathrm{m}}{x}\right)^\alpha & \mbox{for } x \ge x_\mathrm{m}, \\
0 & \mbox{for }x < x_\mathrm{m}.
\end{cases}
\]


{
%%- \subsection{Probability density function}
\begin{itemize}
%When plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.

\item It follows (by differentiation) that the probability density function is
\[
f_X(x)= \begin{cases} \alpha\,\dfrac{x_\mathrm{m}^\alpha}{x^{\alpha+1}} & \mbox{for }x \ge x_\mathrm{m}, \\[12pt] 0 & \mbox{for } x < x_\mathrm{m}. \end{cases} 
\]

% Moments and characteristic function[edit]
\item The expected value of a random variable following a Pareto distribution is
\[
E(X)= \begin{cases} \infty & \mbox{if }\alpha\le 1, \\ \frac{\alpha x_\mathrm{m}}{\alpha-1} & \mbox{if }\alpha>1. \end{cases}
\]


\item The variance of a random variable following a Pareto distribution is
\[
\mathrm{Var}(X)= \begin{cases} \infty & \mbox{if }\alpha\in(1,2], \\ \left(\frac{x_\mathrm{m}}{\alpha-1}\right)^2 \frac{\alpha}{\alpha-2} & \mbox{if }\alpha>2. \end{cases}
(If \alpha\le 1, the variance does not exist.)
\]



\item The Pareto distribution is 
a continuous distribution with the probability density function (pdf):
\[
f(x; \alpha, \beta) = \alpha\beta\alpha / x\alpha+ 1
\]
\item For shape parameter $\alpha > 0$, and location parameter $\beta > 0$, and $\alpha > 0$.
\end{itemize}

\begin{itemize}
\item The Pareto distribution often 
describes the larger compared
to the smaller. 
\item A classic example is that 
80\% of the wealth is owned by 20\% of the population.

% The following graph illustrates how the PDF varies with the location parameter \beta:
\end{itemize}

\newpage
\section{The General Pareto Distribution}

\begin{itemize}
\item As with many other distributions, the Pareto distribution is often generalized by adding a scale parameter. Thus, suppose that Z has the basic Pareto distribution with shape parameter a>0.
\item If b>0, the random variable X=bZ has the Pareto distribution with shape parameter a and scale parameter b. Note that X takes values in the interval $[b, \infty)$.

\item Analogies of the results given above follow easily from basic properties of the scale transformation.

\end{itemize}


%%- \subsection{The Pareto Distribution}

\begin{itemize}
\item The probability density function is
\[ f(x)=abaxa+1,b\leq x< \infty \]
\item The distribution function is
\[F(x)=1-(bx)a,b\leq x< \infty \]


\item The quantile function is
\[F-1(p)=b(1-p)1/a,0\leq p<1\]
\end{itemize}


\noindent \textbf{Cumulative distribution function}
From the definition, the cumulative distribution function of a Pareto random variable with parameters $\alpha$ and xm is
\[F_X(x) = \begin{cases}
1-\left(\frac{x_\mathrm{m}}{x}\right)^\alpha & \mbox{for } x \ge x_\mathrm{m}, \\
0 & \mbox{for }x < x_\mathrm{m}.
\end{cases}
\]


When plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.
\subsection{Probability density function}
It follows (by differentiation) that the probability density function is
\[
f_X(x)= \begin{cases} \alpha\,\dfrac{x_\mathrm{m}^\alpha}{x^{\alpha+1}} & \mbox{for }x \ge x_\mathrm{m}, \\[12pt] 0 & \mbox{for } x < x_\mathrm{m}. \end{cases} 
\]



% Moments and characteristic function[edit]
The expected value of a random variable following a Pareto distribution is
\[
E(X)= \begin{cases} \infty & \text{if }\alpha\le 1, \\ \frac{\alpha x_\mathrm{m}}{\alpha-1} & \text{if }\alpha>1. \end{cases}
\]
The variance of a random variable following a Pareto distribution is
\[
\mathrm{Var}(X)= \begin{cases} \infty & \text{if }\alpha\in(1,2], \\ \left(\frac{x_\mathrm{m}}{\alpha-1}\right)^2 \frac{\alpha}{\alpha-2} & \text{if }\alpha>2. \end{cases}
(If \alpha\le 1, the variance does not exist.)
\]






\subsection{Cumulative distribution function}

From the definition, the cumulative distribution function of a Pareto random variable with parameters $\alpha$ and $x_m$ is
\[F_X(x) = \begin{cases}
1-\left(\frac{x_\mathrm{m}}{x}\right)^\alpha & \mbox{for } x \ge x_\mathrm{m}, \\
0 & \mbox{for }x < x_\mathrm{m}.
\end{cases}
\]


{
%%- \subsection{Probability density function}
\begin{itemize}
%When plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.

\item It follows (by differentiation) that the probability density function is
\[
f_X(x)= \begin{cases} \alpha\,\dfrac{x_\mathrm{m}^\alpha}{x^{\alpha+1}} & \mbox{for }x \ge x_\mathrm{m}, \\[12pt] 0 & \mbox{for } x < x_\mathrm{m}. \end{cases} 
\]

% Moments and characteristic function[edit]
\item The expected value of a random variable following a Pareto distribution is
\[
E(X)= \begin{cases} \infty & \mbox{if }\alpha\le 1, \\ \frac{\alpha x_\mathrm{m}}{\alpha-1} & \mbox{if }\alpha>1. \end{cases}
\]


\item The variance of a random variable following a Pareto distribution is
\[
\mathrm{Var}(X)= \begin{cases} \infty & \mbox{if }\alpha\in(1,2], \\ \left(\frac{x_\mathrm{m}}{\alpha-1}\right)^2 \frac{\alpha}{\alpha-2} & \mbox{if }\alpha>2. \end{cases}
(If \alpha\le 1, the variance does not exist.)
\]



\item The Pareto distribution is 
a continuous distribution with the probability density function (pdf):
\[
f(x; \alpha, \beta) = \alpha\beta\alpha / x\alpha+ 1
\]
\item For shape parameter $\alpha > 0$, and location parameter $\beta > 0$, and $\alpha > 0$.
\end{itemize}




\section{The Pareto Distribution Worked Example}

Suppose the distribution of monthly salaries of full-time workers in the UK has
a Pareto distribution with minimum monthly salary $x_m = 1000$ and concentration
factor $\alpha = 3$. 

\noindent \textbf{The Pareto Distribution}

\begin{enumerate}
\item Calculate the mean monthly salary of UK full-time workers.
\item Calculate the probability that a UK full-time worker earns more than 2000 per month.
\item Calculate the median monthly salary of UK full-time workers.
\end{enumerate}

The expected value of a random variable following a Pareto distribution is
\[E(X)= \begin{cases} \infty & \mbox{if }\alpha\le 1, \\ \frac{\alpha x_\mathrm{m}}{\alpha-1} & \mbox{if }\alpha>1. \end{cases}
\]


Because \textbf{$\alpha$} = $3$, we will use this
{

\[
E(X)= \frac{\alpha x_\mathrm{m}}{\alpha-1}   
\]
}
Recall that $X_m$ = 1000.

The cumulative distribution function of a Pareto random variable with parameters $\alpha$ and $x_m$ is
\[
F_X(x) = \begin{cases}
1-\left(\frac{x_\mathrm{m}}{x}\right)^\alpha & \mbox{for } x \ge x_\mathrm{m}, \\
0 & \mbox{for }x < x_\mathrm{m}.
\end{cases}
\]
Using values for this example:
\[
F_X(x) = \begin{cases}
1-\left(\frac{1000}{x}\right)^3 & \mbox{for } x \ge 1000, \\
0 & \mbox{for }x < 1000.
\end{cases}
\]




Calculate the probability that a UK full-time worker earns \textit{\textbf{more than}} 2000 per month.
{

\[
F_X(x) = \begin{cases}
1-\left(\frac{1000}{x}\right)^3 & \mbox{for } x \ge 1000, \\
0 & \mbox{for }x < 1000.
\end{cases}
\]
}

\noindent \textbf{The Pareto Distribution}

Calculate the median monthly salary of UK full-time workers.

\[ \mbox{Median}: F_X(x) = 0.50\]

{

\[
F_X(x) = \begin{cases}
1-\left(\frac{1000}{x}\right)^3 & \mbox{for } x \ge 1000, \\
0 & \mbox{for }x < 1000.
\end{cases}
\]
}




\[ F_X(x) = 0.5 \qquad \rightarrow \qquad 1-\left(\frac{1000}{x}\right)^3 = 0.50\]


$ \sqrt[3]{0.5} = 0.7937 $



\[\frac{1000}{0.7937} = 1259.92\]





%----------------------------------------- %
\subsection{The Weibull Distribution}






\begin{itemize}
\item The two-parameter Weibull distribution is the most widely used distribution for life data analysis. Apart from the 2-parameter Weibull distribution, the 3-parameter and the 1-parameter Weibull distribution are often used for detailed analysis.
\item 
The 2-parameter Weibull cumulative distribution function (CDF), has the explicit equation:
\end{itemize}

\end{document}
%--------------------------%
%--------------------------%

\subsection{The Weibull Distribution}



\begin{itemize}
\item F(t) = Probability of failure at time t;
\item t = time, cycles, miles, or any appropriate parameter;
\item = characteristic life or scale parameter;
\item = slope or shape parameter.
\end{itemize}


Note that for no other, apart from the Exponential , such an explicit equation is available. Below the F(t) equations for the




%--------------------------%
%--------------------------%

\subsection{Weibull Distribution}

Normal and the Log-Normal distributions.

The Weibull Probability Density Function (PDF) is:



%--------------------------%
%--------------------------%

\subsection{Weibull Distribution}

Weibull Parameters


The two parameters of the 2-Parameter Weibull distribution are:
, the slope or shape parameter, and
, the characteristic life or scale parameter.

%--------------------------%
%--------------------------%

Shape parameter
BETA () shows how the failure rate develops in time. Failure-modes related with Infant Mortality, Random, or Wear-Out have significant different Beta values. is named the shape parameter as it determines which member of the Weibull family of distributions 
is most appropriate. 
Different members have 
different shaped probability 
density functions (PDF

%--------------------------%

Hypergeometric Distribution[edit]
Hypergeometric
Notationh(k) = {{{m \choose k} {{N-m} \choose {n-k}}}\over {N \choose n}} 
Parameters ???
Support ???
Unknown type ???
CDF ???
Mean ???
Median ???
Mode ???
Unknown type ???
Skewness ???
Ex. kurtosis ???
Entropy ???
MGF ???
CF ???
PGF ???
Fisher information ???
The hypergeometric distribution describes the number of successes in a sequence of n draws without replacement from a population of N that contained m total successes.

It's probability mass function is:
\[
f(k) = {{{m \choose k} {{N-m} \choose {n-k}}}\over {N \choose n}}\text{ for all }x \in[0,n]\]
Technically the support for the function is only where x∈[max(0, n+m-N), min(m, n)]. In situations where this range is not [0,n], f(x)=0 since for k>0, {0\choose k}=0.

\subsection{Probability Density Function}
We first check to see that f(x) is a valid pmf. This requires that it is non-negative everywhere and that it's total sum is equal to 1. The first condition is obvious. For the second condition we will start with Vandermonde's identity

$$\sum_{x=0}^n{a \choose x}{b \choose n-x}={a+b \choose n}$$
$$\sum_{x=0}^n{{a \choose x}{b \choose n-x} \over {a+b \choose n}}=1$$
We now see that if a=m and b=N-m that the condition is satisfied.

Mean[edit]
We derive the mean as follows:

$$\operatorname{E}[X] = \sum^n_{x=0} x \cdot f(x;n,m,N)  = \sum^n_{x=0} x \cdot {{{m \choose x} {{N-m} \choose {n-x}}}\over {N \choose n}}$$
\operatorname{E}[X] = 0\cdot {{{m \choose 0} {{N-m} \choose {n-0}}}\over $${N \choose n}}+\sum^n_{x=1} x \cdot {{{m \choose x} {{N-m} \choose {n-x}}}\over {N \choose n}}$$
We use the identity  \binom{a}{b} = \frac{a}{b} \binom{a-1}{b-1} in the denominator.

\operatorname{E}[X] = 0+\sum^n_{x=1} x \cdot {{{m \choose x} {{N-m} \choose {n-x}}}\over {{N \over n}{{N-1} \choose {n-1}}}}
\operatorname{E}[X] = {n \over N}\sum^n_{x=1} x \cdot {{{m \choose x} {{N-m} \choose {n-x}}}\over {{N-1} \choose {n-1}}}
Next we use the identity b \binom{a}{b} = a \binom{a-1}{b-1} in the first binomial of the numerator.

\operatorname{E}[X] = {n \over N}\sum^n_{x=1} {m {{m-1 \choose x-1} {{N-m} \choose {n-x}}}\over {{N-1} \choose {n-1}}}
Next, for the variables inside the sum we define corresponding prime variables that are one less. So N′=N−1, m′=m−1, x′=x−1, n′=n-1.

\operatorname{E}[X] = {m n \over N}\sum^{n'}_{x'=0} {{{m' \choose x '} {{N'-m'} \choose {n'-x'}}}\over {{N'} \choose {n'}}}
\operatorname{E}[X] = {m n \over N}\sum^{n'}_{x'=0} f(x';n',m',N')
Now we see that the sum is the total sum over a Hypergeometric pmf with modified parameters. This is equal to 1. Therefore

\operatorname{E}[X] = {n m\over N}


Variance[edit]
We first determine E(X2).

\operatorname{E}[X^2] = \sum_{x=0}^n f(x;n,m,N) \cdot x^2 = \sum_{x=0}^n {{{m \choose x} {{N-m} \choose {n-x}}}\over {N \choose n}} \cdot x^2
\operatorname{E}[X^2] = {{{m \choose 0} {{N-m} \choose {n-0}}}\over {N \choose n}} \cdot 0^2+\sum_{x=1}^n {{{m \choose x} {{N-m} \choose {n-x}}}\over {N \choose n}} \cdot x^2
\operatorname{E}[X^2] = 0+\sum_{x=1}^n {{m {m-1 \choose x-1} {{N-m} \choose {n-x}}}\over {{N \over n}{{N-1} \choose {n-1}}}} \cdot x
\operatorname{E}[X^2] = {mn \over N} \sum_{x=1}^n {{{m-1 \choose x-1} {{N-m} \choose {n-x}}}\over {{{N-1} \choose {n-1}}}} \cdot x
We use the same variable substitution as when deriving the mean.

\operatorname{E}[X^2] = {mn \over N} \sum_{x'=0}^{n'} {{{m' \choose x'} {{N'-m'} \choose {n'-x'}}}\over {{{N'} \choose {n'}}}} (x'+1)
\operatorname{E}[X^2] = {mn \over N} \left[\sum_{x'=0}^{n'} {{{m' \choose x'} {{N'-m'} \choose {n'-x'}}}\over {{{N'} \choose {n'}}}} x'+\sum_{x'=0}^{n'} {{{m' \choose x'} {{N'-m'} \choose {n'-x'}}}\over {{{N'} \choose {n'}}}}\right]
The first sum is the expected value of a hypergeometric random variable with parameteres (n',m',N'). The second sum is the total sum that random variable's pmf.

\operatorname{E}[X^2] = {mn \over N} \left[{n'm' \over N'}+1\right]
\operatorname{E}[X^2] = {mn \over N} \left[{(n-1)(m-1) \over (N-1)}+1\right]={mn \over N} \left[{{(n-1)(m-1) +(N-1)}\over (N-1)}\right]
We then solve for the variance

$$\operatorname{Var}(X) = \operatorname{E}[X^2]-(\operatorname{E}[X])^2$$
$$\operatorname{Var}(X) = {mn \over N} \left[{{(n-1)(m-1) +(N-1)}\over (N-1)}\right]-\left({mn \over N}\right)^2$$
$$\operatorname{Var}(X) = {Nmn \over N^2} \left[{{(n-1)(m-1) +(N-1)}\over (N-1)}\right]-{(N-1)(mn)^2 \over (N-1)N^2}$$
$$\operatorname{Var}(X) = {nm(N-n)(N-m)\over N^2(N-1)}$$



%=============================================================================== %

\subsection{Discrete Uniform Distribution}

The discrete uniform distribution (not to be confused with the continuous uniform distribution) is where the probability of equally spaced possible values is equal. Mathematically this means that the probability density function is identical for a finite set of evenly spaced points. An example of would be rolling a fair 6-sided die. In this case there are six, equally like probabilities.

%=============================================================================== %

\subsection{Discrete Uniform Distribution}
One common normalization is to restrict the possible values to be integers and the spacing between possibilities to be 1. In this setup, the only two parameters of the function are the minimum value (a), the maximum value (b). (Some even normalize it more, setting a=1.) Let n=b-a+1 be the number of possibilities. The probability density function is then

%=============================================================================== %
f\colon\{a,a+1,\ldots,b-1,b\}\to\R


f\left(x\right)=\frac{1}{n}

Mean[edit]
Let S=\{a,a+1,\ldots,b-1,b\}. The mean (notated as \operatorname{E}[X]) can then be derived as follows:
%=============================================================================== %
\operatorname{E}[X] = \sum_{x\in S} x f(x)=\sum^{n-1}_{i=0}\left(\frac{1}{n}(a+i)\right)
\operatorname{E}[X]= {1 \over n}\left( \sum^{n-1}_{i=0} a + \sum^{n-1}_{i=0} i\right)
Remember that in \sum^{m}_{i=0}i = (m^2 + m)/2
%=============================================================================== %
\operatorname{E}[X]= {1 \over n}\left( na + {(n-1)^2+(n-1) \over 2} \right)
\operatorname{E}[X]= {2na + n^2-2n+1+n-1 \over 2n}
\operatorname{E}[X]= {2a + n-1 \over 2}
\operatorname{E}[X]= {a + b \over 2}
Variance[edit]
%=============================================================================== %
The variance (E[(X-EX)^2]) can be derived:

$$\operatorname{Var}(X) = \operatorname{E}[(X-\operatorname{E}[X])^2] = \sum_{x\in S}f(x)(x-E[X])^2 = \sum^{n-1}_{i=0}\left(\frac{1}{n}\left((a+i)-{a + b \over 2}\right)^2\right)$$
$$\operatorname{Var}(X) = {1 \over n} \sum^{n-1}_{i=0} \left({a+ 2i - b \over 2}\right)^2$$
$$\operatorname{Var}(X) = {1 \over 4n} \sum^{n-1}_{i=0} (a^2+ 4ai-2ab+4i^2-4ib + b^2)$$
$$\operatorname{Var}(X) = {1 \over 4n} \left[\sum^{n-1}_{i=0} (a^2-2ab + b^2)+ \sum^{n-1}_{i=0} (4ai-4ib)+\sum^{n-1}_{i=0}4i^2\right]$$
$$\operatorname{Var}(X) = {1 \over 4n} \left[n(a^2-ab + b^2)+ 4(a-b)\sum^{n-1}_{i=0} i+4\sum^{n-1}_{i=0}i^2\right]$$
Remember that in \sum^{m}_{i=0}i^2 = m(m+1)(2m+1)/6
%=============================================================================== %
$$\operatorname{Var}(X) = {1 \over 4n} \left[n(b-a)^2+ 4(a-b)[(n-1)n/2]+4[(n-1)n(2n-1)/6]\right]$$
$$\operatorname{Var}(X) = {1 \over 4n} \left[n(n-1)^2- 2(n-1)(n-1)n+2(n-1)n(2n-1)/3\right]$$
$$\operatorname{Var}(X) = {1 \over 4} \left[-(n-1)^2+2(n-1)(2n-1)/3\right]$$
$$\operatorname{Var}(X) = {1 \over 12} \left[-3(n-1)^2+2(n-1)(2n-1)\right]$$
$$\operatorname{Var}(X) = {1 \over 12} \left[-3(n^2-2n+1)+2(2n^2-3n+1)\right]$$
$$\operatorname{Var}(X) = {n^2-1 \over 12}$$
\end{document}


\newpage




\section{Negative Binomial Distribution}
Just as the Bernoulli and the Binomial distribution are related in counting the number of successes in 1 or more trials, 
Geometric and the Negative Binomial distribution are related in the number of trials needed to get 1 or more successes.

%===============================================================%
The Negative Binomial distribution refers to the probability of the number of times needed to do something until achieving a fixed number of desired results. For example:

\begin{itemize}
\item How many times will I throw a coin until it lands on heads for the 10th time?
\item How many children will I have when I get my third daughter?
\item How many cards will I have to draw from a pack until I get the second Joker?
\end{itemize}

Just like the Binomial Distribution, the Negative Binomial distribution has two controlling parameters: the probability of success p in any independent test and the desired number of successes m. If a random variable X has Negative Binomial distribution with parameters p and m, its probability mass function is:

\[P(X=n) = {n-1 \choose m-1} p^m (1-p)^{n-m} \mbox{, for } n \ge m.\]
\subsection{Example}
A travelling salesman goes home if he has sold 3 encyclopedias that day. Some days he sells them quickly. Other days he's out till late in the evening. If on the average he sells an encyclopedia at one out of ten houses he approaches, what is the probability of returning home after having visited only 10 houses?

\subsection{Answer:}

The number of trials X is Negative Binomial distributed with parameters p=0.1 and m=3, hence:

P(X=10) = {9 \choose 2} 0.1^3 0.9^7 = 0.0172186884.

%===============================================================%
\subsection{Mean}
The mean can be derived as follows.

\[\operatorname{E}[X] = \sum_i f(x_i)  \cdot x_i = \sum_{x=0}^\infty  {x+r-1 \choose r-1} p^x (1-p)^r  \cdot  x\]
\[\operatorname{E}[X] = {0+r-1 \choose r-1} p^0 \left(1-p\right)^r  \cdot  0 +  \sum_{x=1}^\infty  {x+r-1 \choose r-1} p^x (1-p)^r  \cdot  x\]
\[\operatorname{E}[X] = 0 +  \sum_{x=1}^\infty  {(x+r-1)! \over (r-1)!x!} p^x (1-p)^r  \cdot  x\]
\[\operatorname{E}[X] = {rp \over 1-p}\sum_{x=1}^\infty  {(x+r-1)! \over r!(x-1)!} p^{x-1} (1-p)^{r+1}\]

Now let s = r+1 and w=x-1 inside the summation.

\[\operatorname{E}[X] = {rp \over 1-p}\sum_{w=0}^\infty  {(w+s-1)! \over (s-1)!w!} p^w (1-p)^s]\
[\\operatorname{E}[X] = {rp \over 1-p}\sum_{w=0}^\infty  {w+s-1 \choose s-1} p^w (1-p)^s\]
We see that the summation is the sum over a the complete pmf of a negative binomial random variable distributed NB(s,p), which is 1 (and can be verified by applying Newton's generalized binomial theorem).

\[\operatorname{E}[X] = {rp \over 1-p}]\
%===============================================================%
\section{Variance}
We derive the variance using the following formula:

\[\operatorname{Var}[X] = \operatorname{E}[X^2] - (\operatorname{E}[X])^2\]
We have already calculated E[X] above, so now we will calculate E[X2] and then return to this variance formula:

\[\operatorname{E}[X^2] = \sum_i f(x_i) \cdot x^2]\

\[= \sum_{x=0}^\infty {x+r-1 \choose r-1} p^x (1-p)^r \cdot x^2\]
\[\operatorname{E}[X^2] = 0+\sum_{x=1}^\infty {x+r-1 \choose r-1} p^x (1-p)^r x^2\]
\[\operatorname{E}[X^2] = \sum_{x=1}^\infty {(x+r-1)! \over (r-1)!x!} p^x (1-p)^r x^2\]
\[\operatorname{E}[X^2] = {rp \over 1-p}\sum_{x=1}^\infty {(x+r-1)! \over r!(x-1)!} p^{x-1} (1-p)^{r+1} x\]
Again, let let s = r+1 and w=x-1.

\[\operatorname{E}[X^2] = {rp \over 1-p}\sum_{w=0}^\infty {(w+s-1)! \over (s-1)!w!} p^w (1-p)^s (w+1)\]
%================================%
\[\operatorname{E}[X^2] = {rp \over 1-p}\sum_{w=0}^\infty {w+s-1 \choose s-1} p^w (1-p)^s (w+1)]\
\[\operatorname{E}[X^2] = {rp \over 1-p}\left[\sum_{w=0}^\infty {w+s-1 \choose s-1} p^w (1-p)^s w+\sum_{w=0}^\infty {w+s-1 \choose s-1} p^w (1-p)^s\right]\]
The first summation is the mean of a negative binomial random variable distributed NB(s,p) and the second summation is the complete sum of that variable's pmf.
\[\operatorname{E}[X^2] = {rp \over 1-p}\left[{sp \over 1-p}+1\right]\]
\[\operatorname{E}[X^2] = {rp(1+rp) \over (1-p)^2}\]
We now insert values into the original variance formula.

\[\operatorname{Var}[X] = {rp(1+rp) \over (1-p)^2} - \left({rp \over 1-p}\right)^2\]
\[\operatorname{Var}[X] = {rp \over (1-p)^2} \]


\end{document}
